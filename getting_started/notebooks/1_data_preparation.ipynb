{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Amazon Lookout for Equipment** - Demonstration on an anonymized compressor dataset\n",
    "*Part 1: Data preparation*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "---\n",
    "This repository is initially structured as follow:\n",
    "```\n",
    "/lookout-equipment-demo/getting_started/\n",
    "|\n",
    "├── dataset/                                <<< Original dataset <<<\n",
    "|   ├── labels.csv\n",
    "|   ├── tags_description.csv\n",
    "|   ├── timeranges.txt\n",
    "|   └── timeseries.zip\n",
    "|\n",
    "├── notebooks/\n",
    "|   ├── 1_data_preparation.ipynb            <<< This notebook <<<\n",
    "|   ├── 2_dataset_creation.ipynb\n",
    "|   ├── 3_model_training.ipynb\n",
    "|   ├── 4_model_evaluation.ipynb\n",
    "|   ├── 5_inference_scheduling.ipynb\n",
    "|   └── config.py\n",
    "|\n",
    "└── utils/\n",
    "    ├── aws_matplotlib_light.py\n",
    "    └── lookout_equipment_utils.py\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook configuration update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet --upgrade tqdm tsia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import config\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import shutil\n",
    "import sys\n",
    "import tsia\n",
    "\n",
    "from botocore.client import ClientError\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append('../utils')\n",
    "import lookout_equipment_utils as lookout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET          = config.BUCKET\n",
    "PREFIX_TRAINING = config.PREFIX_TRAINING\n",
    "PREFIX_LABEL    = config.PREFIX_LABEL\n",
    "RAW_DATA        = os.path.join('..', 'dataset')\n",
    "DATA            = os.path.join('..', 'data')\n",
    "LABEL_DATA      = os.path.join(DATA, 'labelled-data')\n",
    "TRAIN_DATA      = os.path.join(DATA, 'training-data', 'expander')\n",
    "\n",
    "os.makedirs(DATA,       exist_ok=True)\n",
    "os.makedirs(LABEL_DATA, exist_ok=True)\n",
    "os.makedirs(TRAIN_DATA, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if BUCKET == '<<YOUR_BUCKET>>':\n",
    "    raise Exception('Please update your Amazon S3 bucket name in the config.py file located at the root of this repository and restart the kernel for this notebook.')\n",
    "    \n",
    "else:\n",
    "    # Check access to the configured bucket:\n",
    "    try:\n",
    "        s3_resource = boto3.resource('s3')\n",
    "        s3_resource.meta.client.head_bucket(Bucket=BUCKET)\n",
    "        print(f'Bucket \"{BUCKET}\" exists')\n",
    "        \n",
    "    # Expose error reason:\n",
    "    except ClientError as error:\n",
    "        error_code = int(error.response['Error']['Code'])\n",
    "        if error_code == 403:\n",
    "            raise Exception(f'Bucket \"{BUCKET}\" is private: access is forbidden!')\n",
    "            \n",
    "        elif error_code == 404:\n",
    "            raise Exception(f'Bucket \"{BUCKET}\" does not exist!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading datasets of interest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeranges_fname = os.path.join(DATA, 'timeranges.txt')\n",
    "shutil.copyfile(os.path.join(RAW_DATA, 'timeranges.txt'), timeranges_fname)\n",
    "with open(timeranges_fname, 'r') as f:\n",
    "    timeranges = f.readlines()\n",
    "    \n",
    "training_start   = pd.to_datetime(timeranges[0][:-1])\n",
    "training_end     = pd.to_datetime(timeranges[1][:-1])\n",
    "evaluation_start = pd.to_datetime(timeranges[2][:-1])\n",
    "evaluation_end   = pd.to_datetime(timeranges[3][:-1])\n",
    "\n",
    "print(f'Training period: from {training_start} to {training_end}')\n",
    "print(f'Evaluation period: from {evaluation_start} to {evaluation_end}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_fname = os.path.join(LABEL_DATA, 'labels.csv')\n",
    "shutil.copyfile(os.path.join(RAW_DATA, 'labels.csv'), labels_fname)\n",
    "labels_df = pd.read_csv(os.path.join(LABEL_DATA, 'labels.csv'), header=None)\n",
    "labels_df[0] = pd.to_datetime(labels_df[0])\n",
    "labels_df[1] = pd.to_datetime(labels_df[1])\n",
    "labels_df.columns = ['start', 'end']\n",
    "labels_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timeseries_fname = os.path.join(RAW_DATA, 'timeseries.zip')\n",
    "!unzip -o $timeseries_fname -d $DATA/training-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_tags_fname = os.path.join(DATA, 'training-data', 'expander.parquet')\n",
    "table = pq.read_table(all_tags_fname)\n",
    "all_tags_df = table.to_pandas()\n",
    "del table\n",
    "\n",
    "print(all_tags_df.shape)\n",
    "all_tags_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tags_description_fname = os.path.join(RAW_DATA, 'tags_description.csv')\n",
    "tags_description_df = pd.read_csv(tags_description_fname)\n",
    "tags_description_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list(tags_description_df.sort_values(by='UOM')['Tag'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset overview\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = pd.to_datetime('2015-04-05 00:00:00')\n",
    "end = evaluation_end\n",
    "\n",
    "df_list = []\n",
    "feature_groups = dict()\n",
    "for f in features:\n",
    "    # Get the unit of measure for the current feature:\n",
    "    uom = str(list(tags_description_df.loc[tags_description_df['Tag'] == f, 'UOM'])[0])\n",
    "    \n",
    "    # We have already some features in this group, add it:\n",
    "    if uom in feature_groups.keys():\n",
    "        feature_groups.update({uom: feature_groups[uom] + [f]})\n",
    "        \n",
    "    # Otherwise, create this group:\n",
    "    else:\n",
    "        feature_groups.update({uom: [f]})\n",
    "    \n",
    "    # Add the dataframe to the list:\n",
    "    current_df = all_tags_df.loc[start:end, [f]]\n",
    "    current_df = current_df.replace(np.nan, 0.0)\n",
    "    df_list.append(current_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag = 'signal-028'\n",
    "tag_df = all_tags_df.loc[start:end, [tag]]\n",
    "tag_df.columns = ['Value']\n",
    "\n",
    "fig, axes = lookout.plot_timeseries(\n",
    "    tag_df, \n",
    "    tag, \n",
    "    fig_width=20, \n",
    "    tag_split=evaluation_start, \n",
    "    labels_df=labels_df\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building and uploading the dataset\n",
    "---\n",
    "We will structure our S3 bucket like this:\n",
    "```\n",
    "s3://sagemaker-lookout-equipment-demo/\n",
    "|\n",
    "├── training-data/\n",
    "|   |\n",
    "|   ├── subsystem-01\n",
    "|   |   └── subsystem-01.csv\n",
    "|   |\n",
    "|   ├── subsystem-02\n",
    "|   |   └── subsystem-02.csv\n",
    "|   |\n",
    "|   ├── ...\n",
    "|   |\n",
    "|   └── subsystem-24\n",
    "|       └── subsystem-24.csv\n",
    "|\n",
    "└── labelled-data/\n",
    "    └── labels.csv\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process each subsystem one by one:\n",
    "components = list(tags_description_df['Subsystem'].unique())\n",
    "progress_bar = tqdm(components)\n",
    "for component in progress_bar:\n",
    "    progress_bar.set_description(f'Component {component}')\n",
    "    progress_bar.refresh()\n",
    "    \n",
    "    # Check if CSV file already exist and do not overwrite it:\n",
    "    component_tags_fname = os.path.join(TRAIN_DATA, f'{component}', f'{component}.csv')\n",
    "    if not os.path.exists(component_tags_fname):\n",
    "        # Build the dataframe with all the signal timeseries for the current subsystem:\n",
    "        component_tags_list = list(tags_description_df[tags_description_df['Subsystem'] == component]['Tag'])\n",
    "        component_tags_df = all_tags_df[component_tags_list]\n",
    "        component_tags_df = component_tags_df.reset_index()\n",
    "        component_tags_df['Timestamp'] = component_tags_df['Timestamp'].dt.strftime('%Y-%m-%dT%H:%M:%S.%f')\n",
    "        \n",
    "        # Save to disk:\n",
    "        os.makedirs(os.path.join(TRAIN_DATA, f'{component}'), exist_ok=True)\n",
    "        component_tags_df.to_csv(component_tags_fname, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploading training dataset to S3:\n",
    "training_src_dir = TRAIN_DATA\n",
    "training_s3_dest_path = f's3://{BUCKET}/{PREFIX_TRAINING}'\n",
    "!aws s3 cp --recursive $training_src_dir $training_s3_dest_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uploading label dataset to S3:\n",
    "label_src_fname = os.path.join(LABEL_DATA, 'labels.csv')\n",
    "label_s3_dest_path = f's3://{BUCKET}/{PREFIX_LABEL}labels.csv'\n",
    "!aws s3 cp $label_src_fname $label_s3_dest_path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
